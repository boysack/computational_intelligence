{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple\n",
    "from random import choice, random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    MAGIC = [2, 7, 6,\n",
    "             9, 5, 1,\n",
    "             4, 3, 8]\n",
    "    \n",
    "    # for the expert agent\n",
    "    CENTER = 5\n",
    "    CORNERS = [2, 6, 8, 4]\n",
    "    SIDES = [7, 1, 3, 9]\n",
    "\n",
    "    def __init__(self, init_player = 0):\n",
    "        self.state = State(set(), set())\n",
    "        self.available_moves = [2, 7, 6,\n",
    "                                9, 5, 1,\n",
    "                                4, 3, 8]\n",
    "\n",
    "        # once this has been introduces, you can even remove from the expert agent the possibility to choose the player (is embedded here)\n",
    "        self.current_player = init_player\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = State(set(), set())\n",
    "        self.available_moves = [2, 7, 6,\n",
    "                                9, 5, 1,\n",
    "                                4, 3, 8]\n",
    "        self.current_player = 0\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def make_move(self, move):\n",
    "        if self.check_available(move) == False or self.game_over:\n",
    "            return False\n",
    "        self.state[self.current_player].add(move)\n",
    "        self.available_moves.remove(move)\n",
    "        self.check_winner()\n",
    "        self.switch_player()\n",
    "        return True\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = (self.current_player + 1) % 2\n",
    "\n",
    "    def check_available(self, move):\n",
    "        return move in self.available_moves\n",
    "\n",
    "    def check_winner(self):\n",
    "        if any(sum(c) == 15 for c in combinations(self.state[self.current_player], 3)):\n",
    "            self.winner = self.current_player\n",
    "            self.game_over = True\n",
    "        elif not self.available_moves:\n",
    "            # ties\n",
    "            self.game_over = True\n",
    "\n",
    "    @property\n",
    "    def reward(self):\n",
    "        if self.winner == 0:\n",
    "            return 1\n",
    "        elif self.winner == 1:\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def print_board(self):\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                i = r * 3 + c\n",
    "                if self.MAGIC[i] in self.state.x:\n",
    "                    print('X', end='')\n",
    "                elif self.MAGIC[i] in self.state.o:\n",
    "                    print('O', end='')\n",
    "                else:\n",
    "                    print('.', end='')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class Agent:\n",
    "  @abstractmethod\n",
    "  def choose_action(self, state: State, available_moves: list) -> int:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expert agent\n",
    "implemented following https://en.wikipedia.org/wiki/Tic-tac-toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertAgent(Agent):\n",
    "  def __init__(self, player = 0):\n",
    "    self.player = player\n",
    "\n",
    "  def choose_action(self, state: State, available_moves: list) -> int:\n",
    "    move = self.win_move(available_moves, state)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    move = self.two_wins_move(available_moves, state)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    move = self.center_move(available_moves)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    move = self.corner_move(available_moves, state)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    return self.side_move(available_moves)\n",
    "\n",
    "  # win else block\n",
    "  def win_move(self, available, state):\n",
    "    move = next((a for a in available for c in combinations(state[self.player], 2) if (sum(c+tuple([a]))==15)), 0)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    return next((a for a in available for c in combinations(state[(self.player+1)%2], 2) if (sum(c+tuple([a]))==15)), 0)\n",
    "\n",
    "  # go to two wins else block two wins\n",
    "  def two_wins_move(self, available, state):\n",
    "    # per ogni available, faccio una copia dello stato, aggiungo l'available, conto i 15, se ce ne sono due la faccio\n",
    "    move = next((a for a in available if sum(sum(c) == 15 for c in combinations(state[self.player].union([a]), 3)) == 2), 0)\n",
    "    if move != 0:\n",
    "      return move\n",
    "    return next((a for a in available if sum(sum(c) == 15 for c in combinations(state[(self.player+1)%2].union([a]), 3)) == 2), 0)\n",
    "\n",
    "  def center_move(self, available):\n",
    "    if TicTacToe.CENTER in available:\n",
    "      return TicTacToe.CENTER\n",
    "    return 0\n",
    "\n",
    "  # play the opposite corner of opponent, else any corner\n",
    "  def corner_move(self, available, state):\n",
    "    for i, c in enumerate(TicTacToe.CORNERS):\n",
    "      if c in state[self.player]:\n",
    "        opp_i = i + 2\n",
    "        if opp_i >= len(TicTacToe.CORNERS):\n",
    "          opp_i -= len(TicTacToe.CORNERS)\n",
    "        if TicTacToe.CORNERS[opp_i] in available:\n",
    "          return TicTacToe.CORNERS[opp_i]\n",
    "    for c in TicTacToe.CORNERS:\n",
    "      if c in available:\n",
    "        return c\n",
    "    return 0\n",
    "\n",
    "  # choose any of the side\n",
    "  def side_move(self, available):\n",
    "    for s in TicTacToe.SIDES:\n",
    "      if s in available:\n",
    "        return s\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "implemented following https://plainenglish.io/blog/building-a-tic-tac-toe-game-with-reinforcement-learning-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, gamma, input_filename = None, output_filename = \"Q\"):\n",
    "        if input_filename:\n",
    "            with open(input_filename, \"rb\") as f:\n",
    "                self.Q = pickle.load(f)\n",
    "        else:\n",
    "            self.Q = {}\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.output_filename = output_filename\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        if (state, action) not in self.Q:\n",
    "            self.Q[(state, action)] = 0.0 # self.Q(( (x, o), action ))\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "    def choose_action(self, state: State, available_moves: list) -> int:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return choice(available_moves)\n",
    "        else:\n",
    "            Q_values = [self.get_value(hashable_state, action) for action in available_moves]\n",
    "            max_Q = max(Q_values)\n",
    "            if Q_values.count(max_Q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
    "                i = choice(best_moves)\n",
    "            else:\n",
    "                i = Q_values.index(max_Q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    # state, action, game.reward, next_state, game.available_moves\n",
    "    def update(self, state, action, reward, next_state, next_available_moves):\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hashable_next_state = (frozenset(next_state.x), frozenset(next_state.o))\n",
    "\n",
    "        next_Q_values = [self.get_value(hashable_next_state, next_action) for next_action in next_available_moves]\n",
    "        max_next_Q = max(next_Q_values) if next_Q_values else 0.0\n",
    "\n",
    "        Q_value = self.get_value(hashable_state, action)\n",
    "        self.Q[(hashable_state, action)] = Q_value + self.alpha * (reward + self.gamma * max_next_Q - Q_value)\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def save_Q(self):\n",
    "        with open(self.output_filename, 'wb') as f:\n",
    "            pickle.dump(self.Q, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  def choose_action(self, _: State, available_moves: list) -> int:\n",
    "    return choice(available_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training Q-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils function to canonize, i.e. consider the symmetry\n",
    "def canonize(state):\n",
    "  pass\n",
    "  # return canonic, transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key(state, action):\n",
    "  return ((frozenset(state.x), frozenset(state.x)), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_train(num_episodes, alpha = .5, epsilon = .8, gamma = .8):\n",
    "    agent_1 = QLearningAgent(alpha, epsilon, gamma)\n",
    "    agent_2 = RandomAgent()\n",
    "    e_range = np.linspace(1, 0.1, num_episodes)\n",
    "    game = TicTacToe()\n",
    "    for step in tqdm(range(num_episodes)):\n",
    "        agent_1.set_epsilon(e_range[step])\n",
    "        while not game.game_over:\n",
    "            state = deepcopy(game.state)\n",
    "            action = agent_1.choose_action(state, game.available_moves)\n",
    "            game.make_move(action)\n",
    "\n",
    "            if game.game_over:\n",
    "                next_state = deepcopy(game.state)\n",
    "                next_actions = game.available_moves\n",
    "                reward = game.reward\n",
    "                agent_1.update(state, action, reward, next_state, next_actions)\n",
    "\n",
    "            else:\n",
    "                reward = game.reward\n",
    "                \n",
    "                a2 = agent_2.choose_action(game.state, game.available_moves)\n",
    "                game.make_move(a2)\n",
    "\n",
    "                if game.game_over:\n",
    "                    reward = game.reward\n",
    "\n",
    "                next_state = deepcopy(game.state)\n",
    "                next_actions = game.available_moves\n",
    "                agent_1.update(state, action, reward, next_state, next_actions)\n",
    "        game.reset()\n",
    "    return agent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1021/100000 [00:00<00:09, 10204.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:07<00:00, 12510.62it/s]\n"
     ]
    }
   ],
   "source": [
    "Q_train(100_000, .5, .8, .8).save_Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 75821.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins: 58.45% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ties = 0\n",
    "wins = 0\n",
    "total = 100_000\n",
    "\n",
    "alpha = .5\n",
    "epsilon = 1\n",
    "gamma = .8\n",
    "\n",
    "players = [QLearningAgent(alpha, epsilon, gamma, input_filename=\"Q\"), RandomAgent()]\n",
    "\n",
    "game = TicTacToe()\n",
    "\n",
    "for steps in tqdm(range(total)):\n",
    "    while not game.game_over:\n",
    "        move = players[game.current_player].choose_action(game.state, game.available_moves)\n",
    "        game.make_move(move)\n",
    "    if game.reward == 1:\n",
    "        wins += 1\n",
    "    elif game.reward == 0:\n",
    "        ties += 1\n",
    "    game.reset()\n",
    "\n",
    "print(f\"wins: {wins/total:.2%} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
